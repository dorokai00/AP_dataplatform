{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import time\n",
    "import urllib.parse\n",
    "from urllib.parse import unquote\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver ## Driver for Firefox, Chrome, Edge, etc.\n",
    "from selenium.webdriver.common.by import By # Mode of locating html elements: ID, CSS_SELECTOR, XPATH, ...\n",
    "from selenium.webdriver.support.select import Select\n",
    "\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper functions navigating website and collecting inbetween data (like links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- FIND COUNTIES IN SH -----\n",
    "def county_search():\n",
    "    #go to start page and make into soup html obj\n",
    "    url = \"https://www.gelbeseiten.de/branchenbuch/staedte/schleswig-holstein/landkreise\"\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    #find all counties\n",
    "    lk = soup.find_all(class_=\"filterlist__item\")\n",
    "    landkreis_stack = [] #create empty stack to put counties into (not a stack tho)\n",
    "    for e in lk:\n",
    "        county_refined = e.text.lower()\n",
    "        county_refined = county_refined.strip()\n",
    "        landkreis_stack.append(county_refined)\n",
    "\n",
    "    return landkreis_stack\n",
    "\n",
    "\n",
    "# ----- FIND CITIES IN PER COUNTY -----\n",
    "def city_search(county_in_question):\n",
    "    #pass relevant counties as argument and get its cities\n",
    "    url = f\"https://www.gelbeseiten.de/branchenbuch/staedte/schleswig-holstein/{county_in_question}\"\n",
    "    #print(url) #remove or comment later\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    places = soup.find_all(class_=\"boxteaser__title\")\n",
    "\n",
    "    city_stack = []\n",
    "    for e in places:\n",
    "        city_stack.append(e.text.lower()) #append all city names (for one county)\n",
    "\n",
    "    return city_stack\n",
    "\n",
    "\n",
    "# ----- FIND BRANCHES PER LOCATION ALPHABETICALLY -----\n",
    "def total_branch_collector(county,city):\n",
    "    #now collect all types of buisness\n",
    "\n",
    "    #first go to county/city\n",
    "    url = f\"https://www.gelbeseiten.de/branchenbuch/staedte/schleswig-holstein/{county}/{city}\"\n",
    "    url = urllib.parse.quote(url, safe=':/')\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    #find links to each letter of alphabet that has businesses\n",
    "    soup_r = soup.find_all(class_=\"alphabetfilter__btn\")\n",
    "    #get links to these subdirectories (And only specific links, theres links to like #a too so filter out)\n",
    "    all_hrefs = [e.get(\"href\") for e in soup_r] \n",
    "    all_hrefs = [e if e!=None else \"get_out\" for e in all_hrefs]\n",
    "    all_hrefs = [e if e[0]=='/' else \"get_out\" for e in all_hrefs]\n",
    "    hrefs = []\n",
    "    for e in all_hrefs[:]:  # Iterate over a copy of the list\n",
    "        if e != \"get_out\":\n",
    "            hrefs.append(f\"https://www.gelbeseiten.de{e}\") #we now have clean links for each letter \n",
    "            #e.g. url = f\"https://www.gelbeseiten.de/branchenbuch/staedte/schleswig-holstein/dithmarschen/albersdorf%20holstein/branchen/b\"\n",
    "    \n",
    "    branchen_refs = []\n",
    "    #now go through all the branchen to get the actual links to buisnesses\n",
    "    for l in hrefs:\n",
    "        html = requests.get(l).text\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "        soup_r = soup.find_all(class_=\"link\") #get the links for each branche\n",
    "        for b in soup_r:\n",
    "            branchen_refs.append(f'https://www.gelbeseiten.de{b.get(\"href\")}')\n",
    "\n",
    "    #some counties are so small they have no unternehmenslist so extra function for those (same as previous)\n",
    "    return branchen_refs\n",
    "\n",
    "\n",
    "# ----- COLLECT INFO IN SH -----\n",
    "def tooSmolNoBranch(county,city):\n",
    "    #apparently here there is no cookies shit anyway\n",
    "    url = f\"https://www.gelbeseiten.de/branchenbuch/staedte/schleswig-holstein/{county}/{city}/unternehmen\"\n",
    "    url = urllib.parse.quote(url, safe=':/')\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    #get all the links to the buisnesses\n",
    "    links = soup.find_all('a',class_='link')\n",
    "    all_links = []\n",
    "    for elm in links:\n",
    "        link_yp = elm.get(\"href\") #get the yellowpage link for 1 buisness\n",
    "        #only keep relevant links\n",
    "        if link_yp.startswith('https://www.gelbeseiten.de/gsbiz'):\n",
    "            all_links.append(link_yp)\n",
    "\n",
    "    return all_links\n",
    "\n",
    "# ----- COLLECT ACTUAL YP BUSINESS INFO -----\n",
    "def collect_buisness_info_nittyGritty(my_buisnesses,my_list):\n",
    "    for elm in my_buisnesses:\n",
    "        time.sleep(1)\n",
    "        #for each element in list of all buisness per branch (in one city)\n",
    "        try:\n",
    "            html = requests.get(elm).text #go there\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            json_soup = soup.find_all(type=\"application/ld+json\") #same as above\n",
    "            if len(json_soup) > 0:\n",
    "                e = json_soup[len(json_soup)-1] #seems to be in last element always (i hope)\n",
    "                e = e.text\n",
    "                data = json.loads(e)\n",
    "            else:\n",
    "                data = {\"business\":elm, \"message\":\"failure - this buisness seems to not be available in json format\"}\n",
    "        except Exception as e:\n",
    "                error_type = type(e).__name__  # Get the name of the exception\n",
    "                error_message = str(e)         # Get the error message\n",
    "                data = {\"website\":elm, \"error_type\":error_type, \"error_message\":error_message}        \n",
    "        \n",
    "        my_list.append(data)\n",
    "\n",
    "### ------ BUSINESSES FROM MY INDEX DB ------\n",
    "def collect_buisness_info_basic(one_link):\n",
    "        try:\n",
    "            html = requests.get(one_link).text #go there\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            json_soup = soup.find_all(type=\"application/ld+json\") #same as above\n",
    "            if len(json_soup) > 0:\n",
    "                e = json_soup[len(json_soup)-1] #seems to be in last element always (i hope)\n",
    "                e = e.text\n",
    "                data = json.loads(e)\n",
    "            else:\n",
    "                data = {\"business\":one_link, \"message\":\"failure - this buisness seems to not be available in json format\"}\n",
    "        except Exception as e:\n",
    "                error_type = type(e).__name__  # Get the name of the exception\n",
    "                error_message = str(e)         # Get the error message\n",
    "                data = {\"website\":one_link, \"error_type\":error_type, \"error_message\":error_message}\n",
    "        #return json object of data\n",
    "        return data\n",
    "\n",
    "\n",
    "# ----- EXPAND RESULT COUNT TO 50+ -----\n",
    "def expand_resultcount(url):\n",
    "    #print(url)\n",
    "    #url = 'https://www.gelbeseiten.de/branchen/friseur/lübeck'\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(url)\n",
    "\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    #manage cookie button\n",
    "    try:\n",
    "        accept_button = driver.find_element(By.XPATH, \"/html/body/div[1]/div[1]/div[2]/span[1]/a\")\n",
    "        accept_button.click()\n",
    "    except:\n",
    "        accept_button = None\n",
    "\n",
    "    #find out how many results in total there are\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    result_totalCount = soup.find_all(id=\"loadMoreGesamtzahl\")\n",
    "    if len(result_totalCount) > 0:\n",
    "        result_totalCount = int(result_totalCount[0].text) #get str w/ number, make into int\n",
    "        result_totalCount = np.round((result_totalCount-50)/10) #because first 50 results are shown\n",
    "    else:\n",
    "        result_totalCount = 0 #f not more than 50 results available just skip while loop\n",
    "\n",
    "    #click load more button as often as needed (each time 10 more results)\n",
    "    load_more_clicked = 1\n",
    "    while load_more_clicked <= result_totalCount:\n",
    "        #driver.execute_script(\"arguments[0].scrollIntoView(true);\", loadMore_button) #thanks chat gtp\n",
    "        #loadMore_button.click()\n",
    "        loadMore_button = driver.find_element(By.CSS_SELECTOR, \"#mod-LoadMore--button\")\n",
    "        time.sleep(1)\n",
    "        driver.execute_script(\"arguments[0].click();\", loadMore_button) #use java script to execute button, since sometimes image ads or headers obscure view (see demo)\n",
    "        load_more_clicked += 1\n",
    "\n",
    "    #now use bs4 again since i already wrote this, just extract all hrefs\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    all_links = soup.find_all(id=re.compile(r'^treffer_\\d+')) #each buisness is a 'treffer' (=hit) followed by _ and some digits\n",
    "    my_buisnesses = [s.find(\"a\")[\"href\"] for s in all_links] #now get all hrefs for the buisnesses and return that list\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return my_buisnesses\n",
    "\n",
    "\n",
    "# ----- COMBINE ALL RESULTS PER BRANCHE + COLLECT EACHs INFOS -----\n",
    "def collect_buisness_infos_2(branchen):\n",
    "    buisnesslist = []\n",
    "    for b in branchen:\n",
    "        #something like this url = \"https://www.gelbeseiten.de/branchen/apotheke/kiel\" is the url then\n",
    "        \n",
    "        #use function with selenium to expand search results from 50 to x, then use bs4 to collect all the hrefs for the buisnesses and return them as a list\n",
    "        my_buisnesses = expand_resultcount(b)\n",
    "        #now go through that list and get info on each buisness\n",
    "        #use this broad try thing, because e.g. timeout errors occured at 126 elements and who knows which other issues might appear (also eduroam hates me)\n",
    "        collect_buisness_info_nittyGritty(my_buisnesses, buisnesslist)\n",
    "    return buisnesslist\n",
    "\n",
    "\n",
    "# ----- X X X -----\n",
    "def collect_business_links(branchen):\n",
    "    buisnesslist = []\n",
    "    for b in branchen:\n",
    "        #something like this url = \"https://www.gelbeseiten.de/branchen/apotheke/kiel\" is the url then\n",
    "        #use function with selenium to expand search results from 50 to x, then use bs4 to collect all the hrefs for the buisnesses and return them as a list\n",
    "        business_found = expand_resultcount(b)\n",
    "        buisnesslist.extend(business_found)\n",
    "    return buisnesslist\n",
    "\n",
    "\n",
    "# ---- MAKE YP LINK TO BUSINESSES INTO JSON OBJECT TO DUMP INTO DB -----\n",
    "def links_to_index_obj(one_link): \n",
    "    index_object = {\"@id\":one_link, \"data_collected\":\"false\"}\n",
    "    return index_object\n",
    "\n",
    "\n",
    "# --- CHECK IF THIS BUSINESS HAS ALREADY BEEN NOTED ---\n",
    "def check_indices(collection,link):\n",
    "    available_buiz = [x for x in collection.find({'@id':f'{link}'},{'_id':0})]\n",
    "    if len(available_buiz) >= 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# ----- SET UP MONGO -----\n",
    "def set_up_mongo(client_str,database_str,collection_str):\n",
    "    client = MongoClient(client_str) #connect to mongodb client\n",
    "    db = client[database_str] #connect to database\n",
    "\n",
    "    existing_collections = db.list_collection_names() #check that dbs collections\n",
    "    if collection_str not in existing_collections:\n",
    "        db.create_collection(collection_str) #create collection if needed\n",
    "    \n",
    "    my_collection = db[collection_str] #connect to collection\n",
    "\n",
    "    return my_collection\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTUAL SET UP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First collect all links to all businesses in SH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dithmarschen - done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lk_stack = county_search()\n",
    "lk_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = city_search('pinneberg')\n",
    "city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lk = \"pinneberg\"\n",
    "\n",
    "cities_next = ['schenefeld bezirk hamburg',\n",
    " 'seester',\n",
    " 'seestermühe',\n",
    " 'seeth-ekholt',\n",
    " 'tangstedt kreis pinneberg',\n",
    " 'tornesch',\n",
    " 'uetersen',\n",
    " 'wedel',\n",
    " 'westerhorn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_indices = set_up_mongo('mongodb://localhost:27017/','webscraping_dataLabKiel','yellow_pages_index')\n",
    "\n",
    "pattern = r\"[)(]\" #take care of unclean stuff, look like only brakcets are an issue\n",
    "city_chic = [re.sub(pattern, '', e) for e in cities_next]\n",
    "for c in city_chic:\n",
    "    sh_unternehmen_links = [] #set up collector\n",
    "    my_branch_list = total_branch_collector(lk,c) #get list of all links of all the branchen which are available in 1 city alphabetically\n",
    "    if my_branch_list == []:\n",
    "        sh_unternehmen_links = tooSmolNoBranch(lk,c) #get list of all href of buisness for that place (if so small theres no branchen split up)\n",
    "        print(f\"links for 1 SMALL city collected! city/landkreis: {c}/{lk}\")\n",
    "    else:\n",
    "        sh_unternehmen_links = collect_business_links(my_branch_list)\n",
    "        print(f\"links for 1 city collected! city/landkreis: {c}/{lk}\")\n",
    "\n",
    "    #and then for each city, go through list of all business links, check if they are in index db, and only if not put them in there\n",
    "    for e in sh_unternehmen_links:\n",
    "        #only if we assume that in one city businesses arent listed twice, then we can reduce the querying, but since the point literally is not to get duplicates, lets not\n",
    "        if check_indices(collection_indices,e) == False:\n",
    "            d = links_to_index_obj(e)\n",
    "            collection_indices.insert_one(d)\n",
    "    print(f\"links also inserted for city/landkreis: {c}/{lk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "run this over night!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"collection_indices = set_up_mongo('mongodb://localhost:27017/','webscraping_dataLabKiel','yellow_pages_index')\n",
    "\n",
    "for lk in ['pinneberg','plön','rendsburg-eckernförde','schleswig-flensburg','segeberg','steinburg','stormarn']:\n",
    "    cities_next = city_search(lk)\n",
    "    pattern = r\"[)(]\" #take care of unclean stuff, look like only brakcets are an issue\n",
    "    city_chic = [re.sub(pattern, '', e) for e in cities_next]\n",
    "    for c in city_chic:\n",
    "        sh_unternehmen_links = [] #set up collector\n",
    "        my_branch_list = total_branch_collector(lk,c) #get list of all links of all the branchen which are available in 1 city alphabetically\n",
    "        if my_branch_list == []:\n",
    "            sh_unternehmen_links = tooSmolNoBranch(lk,c) #get list of all href of buisness for that place (if so small theres no branchen split up)\n",
    "            print(f\"links for 1 SMALL city collected! city/landkreis: {c}/{lk}\")\n",
    "        else:\n",
    "            sh_unternehmen_links = collect_business_links(my_branch_list)\n",
    "            print(f\"links for 1 city collected! city/landkreis: {c}/{lk}\")\n",
    "\n",
    "        #and then for each city, go through list of all business links, check if they are in index db, and only if not put them in there\n",
    "        for e in sh_unternehmen_links:\n",
    "            #only if we assume that in one city businesses arent listed twice, then we can reduce the querying, but since the point literally is not to get duplicates, lets not\n",
    "            if check_indices(collection_indices,e) == False:\n",
    "                d = links_to_index_obj(e)\n",
    "                collection_indices.insert_one(d)\n",
    "        print(f\"links also inserted for city/landkreis: {c}/{lk}\\n\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_indices = set_up_mongo('mongodb://localhost:27017/','webscraping_dataLabKiel','yellow_pages_index')\n",
    "\n",
    "#then do the same for kreisfreie städe\n",
    "for kfs in [\"flensburg\",\"neumünster\",\"lübeck\",\"kiel\"]:\n",
    "    my_branch_list = total_branch_collector(\"kreisfrei\",kfs)\n",
    "    sh_unternehmen_links = collect_business_links(my_branch_list)\n",
    "    for e in sh_unternehmen_links:\n",
    "        if check_indices(collection_indices,e) == False:\n",
    "            d = links_to_index_obj(e)\n",
    "            collection_indices.insert_one(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then go through new index db and into a second collection YP_DATA_CLEAN insert only data for businesses we have once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_indices = set_up_mongo('mongodb://localhost:27017/','webscraping_dataLabKiel','yellow_pages_index')\n",
    "collection_yp_clean = set_up_mongo('mongodb://localhost:27017/','webscraping_dataLabKiel','yellow_pages_clean_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect all buisnesses where the flag attr hasnt been set to true\n",
    "available_buiz_not_collected = [x for x in collection_indices.find({\"data_collected\":\"false\"},{'_id':0})]\n",
    "for a in available_buiz_not_collected:\n",
    "    #collect data for one link\n",
    "    data = collect_buisness_info_basic(a)\n",
    "    collection_yp_clean.insert_one(data)\n",
    "\n",
    "    #set flag in index list to true\n",
    "    query = {\"@id\": f\"{a}\"}\n",
    "    update = {\"$set\": {\"data_collected\": \"true\"}}\n",
    "    collection_indices.update_one(query, update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_mongo(client_str,database_str,collection_str):\n",
    "    client = MongoClient(client_str) #connect to mongodb client\n",
    "    db = client[database_str] #connect to database\n",
    "\n",
    "    existing_collections = db.list_collection_names() #check that dbs collections\n",
    "    if collection_str not in existing_collections:\n",
    "        db.create_collection(collection_str) #create collection if needed\n",
    "    \n",
    "    my_collection = db[collection_str] #connect to collection\n",
    "\n",
    "    return my_collection\n",
    "\n",
    "\n",
    "collection_yp_clean = set_up_mongo('mongodb://localhost:27017/','webscraping_dataLabKiel','yellow_pages_clean_v2')\n",
    "\n",
    "for x in collection_yp_clean.find():\n",
    "    id = x[\"_id\"]\n",
    "    try:\n",
    "        lat = float(x[\"latitude\"])\n",
    "        lon = float(x[\"longitude\"])\n",
    "    except:\n",
    "        lat = None\n",
    "        lon = None\n",
    "    x.update({\"lat\":lat,\"lon\":lon})\n",
    "    collection_yp_clean.update_one({\"_id\":id},{\"$set\":{\"lat\":lat,\"lon\":lon}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMALL TRIAL SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_indices = set_up_mongo('mongodb://localhost:27017/','webscraping_dataLabKiel','yellow_pages_index')\n",
    "\n",
    "#get all counties\n",
    "lk = \"dithmarschen\"\n",
    "city_stack = ['arkebek','averlak','bargenstedt','barkenholm','barlt']\n",
    "\n",
    "\n",
    "pattern = r\"[)(]\" #take care of unclean stuff, look like only brakcets are an issue\n",
    "city_chic = [re.sub(pattern, '', e) for e in city_stack]\n",
    "\n",
    "for c in city_chic:\n",
    "    sh_unternehmen_links = [] #set up collector\n",
    "    my_branch_list = total_branch_collector(lk,c) #get list of all links of all the branchen which are available in 1 city alphabetically\n",
    "    if my_branch_list == []:\n",
    "        sh_unternehmen_links = tooSmolNoBranch(lk,c) #get list of all href of buisness for that place (if so small theres no branchen split up)\n",
    "        print(f\"links for 1 SMALL city collected! city/landkreis: {c}/{lk}\")\n",
    "    else:\n",
    "        sh_unternehmen_links = collect_business_links(my_branch_list)\n",
    "        print(f\"links for 1 city collected! city/landkreis: {c}/{lk}\")\n",
    "\n",
    "    #and then for each city, go through list of all business links, check if they are in index db, and only if not put them in there\n",
    "    for e in sh_unternehmen_links:\n",
    "        if check_indices(collection_indices,e) == False:\n",
    "            d = links_to_index_obj(e)\n",
    "            collection_indices.insert_one(d)\n",
    "    print(f\"links also inserted for city/landkreis: {c}/{lk}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_indices = set_up_mongo('mongodb://localhost:27017/','webscraping_dataLabKiel','yellow_pages_index')\n",
    "collection_yp_clean = set_up_mongo('mongodb://localhost:27017/','webscraping_dataLabKiel','yp_data_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect all buisnesses where the flag attr hasnt been set to true\n",
    "available_buiz_not_collected = [x for x in collection_indices.find({\"data_collected\":\"false\"},{'_id':0})]\n",
    "for a in available_buiz_not_collected:\n",
    "    #collect data for one link\n",
    "    link = a[\"@id\"]\n",
    "    data = collect_buisness_info_basic(link)\n",
    "    collection_yp_clean.insert_one(data)\n",
    "\n",
    "    #set flag in index list to true\n",
    "    query = {\"@id\": f\"{e}\"}\n",
    "    update = {\"$set\": {\"data_collected\": \"true\"}}\n",
    "    collection_indices.update_one(query, update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some issues:\n",
    "\n",
    "\n",
    "\n",
    "- **some business adress not actualy in locality where i found it** <br>\n",
    "but whats really weird is that when you go here (one of the cities in dithmarschen) https://www.gelbeseiten.de/branchenbuch/staedte/schleswig-holstein/dithmarschen/bargenstedt/unternehmen what you see is for example the Fritz Käppner GmbH. and its adresses is in surprise: Nuermberg. wtf\n",
    "- **number of businesses** <br> apparently theres 123000+ businesses in SH, so maybe use some hash function so i dont have to query them all https://www.schleswig-holstein.de/DE/landesregierung/themen/wirtschaft/mittelstand-handwerk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# update data in collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_yp_clean = set_up_mongo('mongodb://localhost:27017/','webscraping_dataLabKiel','yp_data_clean')\n",
    "\n",
    "for x in collection_yp_clean.find():\n",
    "    id = x[\"_id\"]\n",
    "    try:\n",
    "        lat = float(x[\"latitude\"])\n",
    "        lon = float(x[\"longitude\"])\n",
    "    except:\n",
    "        lat = None\n",
    "        lon = None\n",
    "    x.update({\"lat\":lat,\"lon\":lon})\n",
    "    collection_yp_clean.update_one({\"_id\":id},{\"$set\":{\"lat\":lat,\"lon\":lon}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datalab_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
