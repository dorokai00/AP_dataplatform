{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import time\n",
    "import urllib.parse\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver ## Driver for Firefox, Chrome, Edge, etc.\n",
    "from selenium.webdriver.common.by import By # Mode of locating html elements: ID, CSS_SELECTOR, XPATH, ...\n",
    "from selenium.webdriver.support.select import Select\n",
    "\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- these first 3 are like batch collections right, so no delay necessary -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- FIND COUNTIES IN SH -----\n",
    "def county_search():\n",
    "    #go to start page and make into soup html obj\n",
    "    url = \"https://www.gelbeseiten.de/branchenbuch/staedte/schleswig-holstein/landkreise\"\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    #find all counties\n",
    "    lk = soup.find_all(class_=\"filterlist__item\")\n",
    "    landkreis_stack = [] #create empty stack to put counties into (not a stack tho)\n",
    "    for e in lk:\n",
    "        county_refined = e.text.lower()\n",
    "        county_refined = county_refined.strip()\n",
    "        landkreis_stack.append(county_refined)\n",
    "\n",
    "    return landkreis_stack\n",
    "\n",
    "\n",
    "# ----- FIND CITIES IN PER COUNTY -----\n",
    "def city_search(county_in_question):\n",
    "    #pass relevant counties as argument and get its cities\n",
    "    url = f\"https://www.gelbeseiten.de/branchenbuch/staedte/schleswig-holstein/{county_in_question}\"\n",
    "    #print(url) #remove or comment later\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    places = soup.find_all(class_=\"boxteaser__title\")\n",
    "\n",
    "    city_stack = []\n",
    "    for e in places:\n",
    "        city_stack.append(e.text.lower()) #append all city names (for one county)\n",
    "\n",
    "    return city_stack\n",
    "\n",
    "\n",
    "# ----- FIND BRANCHES PER LOCATION ALPHABETICALLY -----\n",
    "def total_branch_collector(county,city):\n",
    "    #now collect all types of buisness\n",
    "\n",
    "    #first go to county/city\n",
    "    url = f\"https://www.gelbeseiten.de/branchenbuch/staedte/schleswig-holstein/{county}/{city}\"\n",
    "    url = urllib.parse.quote(url, safe=':/')\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    #find links to each letter of alphabet that has businesses\n",
    "    soup_r = soup.find_all(class_=\"alphabetfilter__btn\")\n",
    "    #get links to these subdirectories (And only specific links, theres links to like #a too so filter out)\n",
    "    all_hrefs = [e.get(\"href\") for e in soup_r] \n",
    "    all_hrefs = [e if e!=None else \"get_out\" for e in all_hrefs]\n",
    "    all_hrefs = [e if e[0]=='/' else \"get_out\" for e in all_hrefs]\n",
    "    hrefs = []\n",
    "    for e in all_hrefs[:]:  # Iterate over a copy of the list\n",
    "        if e != \"get_out\":\n",
    "            hrefs.append(f\"https://www.gelbeseiten.de{e}\") #we now have clean links for each letter \n",
    "            #e.g. url = f\"https://www.gelbeseiten.de/branchenbuch/staedte/schleswig-holstein/dithmarschen/albersdorf%20holstein/branchen/b\"\n",
    "    \n",
    "    branchen_refs = []\n",
    "    #now go through all the branchen to get the actual links to buisnesses\n",
    "    for l in hrefs:\n",
    "        html = requests.get(l).text\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "        soup_r = soup.find_all(class_=\"link\") #get the links for each branche\n",
    "        for b in soup_r:\n",
    "            branchen_refs.append(f'https://www.gelbeseiten.de{b.get(\"href\")}')\n",
    "\n",
    "    #some counties are so small they have no unternehmenslist so extra function for those (same as previous)\n",
    "    return branchen_refs\n",
    "\n",
    "\n",
    "# ----- COLLECT INFO IN SH -----\n",
    "def tooSmolNoBranch(county,city):\n",
    "    #apparently here there is no cookies shit anyway\n",
    "    url = f\"https://www.gelbeseiten.de/branchenbuch/staedte/schleswig-holstein/{county}/{city}/unternehmen\"\n",
    "    url = urllib.parse.quote(url, safe=':/')\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    #get all the links to the buisnesses\n",
    "    links = soup.find_all('a',class_='link')\n",
    "    all_links = []\n",
    "    for elm in links:\n",
    "        link_yp = elm.get(\"href\") #get the yellowpage link for 1 buisness\n",
    "        #only keep relevant links\n",
    "        if link_yp.startswith('https://www.gelbeseiten.de/gsbiz'):\n",
    "            all_links.append(link_yp)\n",
    "\n",
    "    return all_links\n",
    "\n",
    "# ----- COLLECT ACTUAL YP BUSINESS INFO -----\n",
    "def collect_buisness_info_nittyGritty(my_buisnesses):\n",
    "    my_list = []\n",
    "    for elm in my_buisnesses:\n",
    "        time.sleep(1)\n",
    "        #for each element in list of all buisness per branch (in one city)\n",
    "        try:\n",
    "            html = requests.get(elm).text #go there\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            json_soup = soup.find_all(type=\"application/ld+json\") #same as above\n",
    "            if len(json_soup) > 0:\n",
    "                e = json_soup[len(json_soup)-1] #seems to be in last element always (i hope)\n",
    "                e = e.text\n",
    "                data = json.loads(e)\n",
    "            else:\n",
    "                data = {\"business\":elm, \"message\":\"failure - this buisness seems to not be available in json format\"}\n",
    "        except Exception as e:\n",
    "                error_type = type(e).__name__  # Get the name of the exception\n",
    "                error_message = str(e)         # Get the error message\n",
    "                data = {\"website\":elm, \"error_type\":error_type, \"error_message\":error_message}        \n",
    "        my_list.append(data)\n",
    "    return my_list\n",
    "\n",
    "\n",
    "# ----- EXPAND RESULT COUNT TO 50+ -----\n",
    "def expand_resultcount(url):\n",
    "\n",
    "    #url = 'https://www.gelbeseiten.de/branchen/friseur/lÃ¼beck'\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(url)\n",
    "\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    #manage cookie button\n",
    "    try:\n",
    "        accept_button = driver.find_element(By.XPATH, \"/html/body/div[1]/div[1]/div[2]/span[1]/a\")\n",
    "        accept_button.click()\n",
    "    except:\n",
    "        accept_button = None\n",
    "\n",
    "    #find out how many results in total there are\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    result_totalCount = soup.find_all(id=\"loadMoreGesamtzahl\")\n",
    "    if len(result_totalCount) > 0:\n",
    "        result_totalCount = int(result_totalCount[0].text) #get str w/ number, make into int\n",
    "        result_totalCount = np.round((result_totalCount-50)/10) #because first 50 results are shown\n",
    "    else:\n",
    "        result_totalCount = 0 #f not more than 50 results available just skip while loop\n",
    "\n",
    "    #click load more button as often as needed (each time 10 more results)\n",
    "    load_more_clicked = 1\n",
    "    while load_more_clicked <= result_totalCount:\n",
    "        #driver.execute_script(\"arguments[0].scrollIntoView(true);\", loadMore_button) #thanks chat gtp\n",
    "        #loadMore_button.click()\n",
    "        loadMore_button = driver.find_element(By.CSS_SELECTOR, \"#mod-LoadMore--button\")\n",
    "        time.sleep(2)\n",
    "        driver.execute_script(\"arguments[0].click();\", loadMore_button) #use java script to execute button, since sometimes image ads or headers obscure view (see demo)\n",
    "        load_more_clicked += 1\n",
    "\n",
    "    #now use bs4 again since i already wrote this, just extract all hrefs\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    all_links = soup.find_all(id=re.compile(r'^treffer_\\d+')) #each buisness is a 'treffer' (=hit) followed by _ and some digits\n",
    "    my_buisnesses = [s.find(\"a\")[\"href\"] for s in all_links] #now get all hrefs for the buisnesses and return that list\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return my_buisnesses\n",
    "\n",
    "\n",
    "# ----- COMBINE ALL RESULTS PER BRANCHE + COLLECT EACHs INFOS -----\n",
    "def collect_buisness_infos_2(branche):\n",
    "    #something like this url = \"https://www.gelbeseiten.de/branchen/apotheke/kiel\" is the url then\n",
    "    #use function with selenium to expand search results from 50 to x, then use bs4 to collect all the hrefs for the buisnesses and return them as a list\n",
    "    my_buisnesses = expand_resultcount(branche)\n",
    "    #now go through that list and get info on each buisness\n",
    "    #use this broad try thing, because e.g. timeout errors occured at 126 elements and who knows which other issues might appear (also eduroam hates me)\n",
    "    buisnesslist = collect_buisness_info_nittyGritty(my_buisnesses)\n",
    "    return buisnesslist\n",
    "\n",
    "\n",
    "# ----- SET UP MONGO -----\n",
    "def set_up_mongo(client_str,database_str,collection_str):\n",
    "    client = MongoClient(client_str) #connect to mongodb client\n",
    "    db = client[database_str] #connect to database\n",
    "\n",
    "    existing_collections = db.list_collection_names() #check that dbs collections\n",
    "    if collection_str not in existing_collections:\n",
    "        db.create_collection(collection_str) #create collection if needed\n",
    "    \n",
    "    my_collection = db[collection_str] #connect to collection\n",
    "\n",
    "    return my_collection\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_branch_list = total_branch_collector(\"kreisfrei\",\"kiel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.gelbeseiten.de/branchen/transportunternehmen/kiel\n",
      "https://www.gelbeseiten.de/branchen/trockenbau/kiel\n",
      "https://www.gelbeseiten.de/branchen/t%c3%a4schner/kiel\n",
      "https://www.gelbeseiten.de/branchen/t%c3%bcren/kiel\n",
      "https://www.gelbeseiten.de/branchen/umzugsfirma/kiel\n",
      "https://www.gelbeseiten.de/branchen/umzugsunternehmen/kiel\n",
      "https://www.gelbeseiten.de/branchen/umz%c3%bcge/kiel\n",
      "https://www.gelbeseiten.de/branchen/unternehmensberatung/kiel\n",
      "https://www.gelbeseiten.de/branchen/urologe/kiel\n",
      "https://www.gelbeseiten.de/branchen/venerologie/kiel\n",
      "https://www.gelbeseiten.de/branchen/veranstaltungstechnik/kiel\n",
      "https://www.gelbeseiten.de/branchen/vereine/kiel\n",
      "https://www.gelbeseiten.de/branchen/verlag/kiel\n",
      "https://www.gelbeseiten.de/branchen/vermessungsb%c3%bcro/kiel\n",
      "https://www.gelbeseiten.de/branchen/verm%c3%b6gensberatung/kiel\n",
      "https://www.gelbeseiten.de/branchen/verpackungsmaterial/kiel\n",
      "https://www.gelbeseiten.de/branchen/verputzer/kiel\n",
      "https://www.gelbeseiten.de/branchen/versicherungen/kiel\n",
      "https://www.gelbeseiten.de/branchen/versicherungsmakler/kiel\n",
      "https://www.gelbeseiten.de/branchen/vinothek/kiel\n",
      "https://www.gelbeseiten.de/branchen/waffenladen/kiel\n",
      "https://www.gelbeseiten.de/branchen/wasserschaden/kiel\n",
      "https://www.gelbeseiten.de/branchen/webdesign/kiel\n",
      "https://www.gelbeseiten.de/branchen/weinhandlung/kiel\n",
      "https://www.gelbeseiten.de/branchen/wellness/kiel\n",
      "https://www.gelbeseiten.de/branchen/werbeagentur/kiel\n",
      "https://www.gelbeseiten.de/branchen/werkzeuge/kiel\n",
      "https://www.gelbeseiten.de/branchen/winterdienst/kiel\n",
      "https://www.gelbeseiten.de/branchen/wintergarten/kiel\n",
      "https://www.gelbeseiten.de/branchen/wirtschaftspr%c3%bcfer/kiel\n",
      "https://www.gelbeseiten.de/branchen/w%c3%a4scherei/kiel\n",
      "https://www.gelbeseiten.de/branchen/yogastudio/kiel\n",
      "https://www.gelbeseiten.de/branchen/zahnarzt/kiel\n",
      "https://www.gelbeseiten.de/branchen/zahntechniker/kiel\n",
      "https://www.gelbeseiten.de/branchen/zaunbau/kiel\n",
      "https://www.gelbeseiten.de/branchen/zeitarbeit/kiel\n",
      "https://www.gelbeseiten.de/branchen/zeitschriften/kiel\n",
      "https://www.gelbeseiten.de/branchen/zeitungsverlag/kiel\n",
      "https://www.gelbeseiten.de/branchen/zimmerei/kiel\n",
      "https://www.gelbeseiten.de/branchen/zoohandlung/kiel\n",
      "https://www.gelbeseiten.de/branchen/zweiradmechaniker/kiel\n"
     ]
    }
   ],
   "source": [
    "#collection = set_up_mongo('mongodb://localhost:27017/','webscraping_dataLabKiel','yellow_pages')\n",
    "collection = set_up_mongo('mongodb://localhost:27017','sh_data_collection','yp_kiel') #mongodb://mongodb:27017\n",
    "\n",
    "for shu in my_branch_list_smol:\n",
    "    print(shu)\n",
    "    sh_unternehmen = collect_buisness_infos_2(shu) \n",
    "    collection.insert_many(sh_unternehmen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# update location naming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_mongo(client_str,database_str,collection_str):\n",
    "    client = MongoClient(client_str) #connect to mongodb client\n",
    "    db = client[database_str] #connect to database\n",
    "\n",
    "    existing_collections = db.list_collection_names() #check that dbs collections\n",
    "    if collection_str not in existing_collections:\n",
    "        db.create_collection(collection_str) #create collection if needed\n",
    "    \n",
    "    my_collection = db[collection_str] #connect to collection\n",
    "\n",
    "    return my_collection\n",
    "\n",
    "\n",
    "collection_yp_clean = set_up_mongo('mongodb://localhost:27017/','sh_data_collection','yp_kiel')\n",
    "\n",
    "for x in collection_yp_clean.find():\n",
    "    id = x[\"_id\"]\n",
    "    try:\n",
    "        lat = float(x[\"latitude\"])\n",
    "        lon = float(x[\"longitude\"])\n",
    "    except:\n",
    "        lat = None\n",
    "        lon = None\n",
    "    x.update({\"lat\":lat,\"lon\":lon})\n",
    "    collection_yp_clean.update_one({\"_id\":id},{\"$set\":{\"lat\":lat,\"lon\":lon}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some issues:\n",
    "\n",
    "\n",
    "\n",
    "- **some business adress not actualy in locality where i found it** <br>\n",
    "but whats really weird is that when you go here (one of the cities in dithmarschen) https://www.gelbeseiten.de/branchenbuch/staedte/schleswig-holstein/dithmarschen/bargenstedt/unternehmen what you see is for example the Fritz KÃ¤ppner GmbH. and its adresses is in surprise: Nuermberg. wtf\n",
    "- **number of businesses** <br> apparently theres 123000+ businesses in SH, so maybe use some hash function so i dont have to query them all https://www.schleswig-holstein.de/DE/landesregierung/themen/wirtschaft/mittelstand-handwerk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datalab_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
