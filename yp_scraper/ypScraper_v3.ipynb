{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import time\n",
    "import urllib.parse\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver ## Driver for Firefox, Chrome, Edge, etc.\n",
    "from selenium.webdriver.common.by import By # Mode of locating html elements: ID, CSS_SELECTOR, XPATH, ...\n",
    "from selenium.webdriver.support.select import Select\n",
    "\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these first 3 are like batch collections right, so no delay necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- FIND COUNTIES IN SH -----\n",
    "def county_search():\n",
    "    #go to start page and make into soup html obj\n",
    "    url = \"https://www.gelbeseiten.de/branchenbuch/staedte/schleswig-holstein/landkreise\"\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    #find all counties\n",
    "    lk = soup.find_all(class_=\"filterlist__item\")\n",
    "    landkreis_stack = [] #create empty stack to put counties into (not a stack tho)\n",
    "    for e in lk:\n",
    "        county_refined = e.text.lower()\n",
    "        county_refined = county_refined.strip()\n",
    "        landkreis_stack.append(county_refined)\n",
    "\n",
    "    return landkreis_stack\n",
    "\n",
    "\n",
    "# ----- FIND CITIES IN PER COUNTY -----\n",
    "def city_search(county_in_question):\n",
    "    #pass relevant counties as argument and get its cities\n",
    "    url = f\"https://www.gelbeseiten.de/branchenbuch/staedte/schleswig-holstein/{county_in_question}\"\n",
    "    #print(url) #remove or comment later\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    places = soup.find_all(class_=\"boxteaser__title\")\n",
    "\n",
    "    city_stack = []\n",
    "    for e in places:\n",
    "        city_stack.append(e.text.lower()) #append all city names (for one county)\n",
    "\n",
    "    return city_stack\n",
    "\n",
    "\n",
    "# ----- FIND BRANCHES PER LOCATION ALPHABETICALLY -----\n",
    "def total_branch_collector(county,city):\n",
    "    #now collect all types of buisness\n",
    "\n",
    "    #first go to county/city\n",
    "    url = f\"https://www.gelbeseiten.de/branchenbuch/staedte/schleswig-holstein/{county}/{city}\"\n",
    "    url = urllib.parse.quote(url, safe=':/')\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    #find links to each letter of alphabet that has businesses\n",
    "    soup_r = soup.find_all(class_=\"alphabetfilter__btn\")\n",
    "    #get links to these subdirectories (And only specific links, theres links to like #a too so filter out)\n",
    "    all_hrefs = [e.get(\"href\") for e in soup_r] \n",
    "    all_hrefs = [e if e!=None else \"get_out\" for e in all_hrefs]\n",
    "    all_hrefs = [e if e[0]=='/' else \"get_out\" for e in all_hrefs]\n",
    "    hrefs = []\n",
    "    for e in all_hrefs[:]:  # Iterate over a copy of the list\n",
    "        if e != \"get_out\":\n",
    "            hrefs.append(f\"https://www.gelbeseiten.de{e}\") #we now have clean links for each letter \n",
    "            #e.g. url = f\"https://www.gelbeseiten.de/branchenbuch/staedte/schleswig-holstein/dithmarschen/albersdorf%20holstein/branchen/b\"\n",
    "    \n",
    "    branchen_refs = []\n",
    "    #now go through all the branchen to get the actual links to buisnesses\n",
    "    for l in hrefs:\n",
    "        html = requests.get(l).text\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "        soup_r = soup.find_all(class_=\"link\") #get the links for each branche\n",
    "        for b in soup_r:\n",
    "            branchen_refs.append(f'https://www.gelbeseiten.de{b.get(\"href\")}')\n",
    "\n",
    "    #some counties are so small they have no unternehmenslist so extra function for those (same as previous)\n",
    "    return branchen_refs\n",
    "\n",
    "\n",
    "# ----- COLLECT INFO IN SH -----\n",
    "def tooSmolNoBranch(county,city):\n",
    "    #apparently here there is no cookies shit anyway\n",
    "    url = f\"https://www.gelbeseiten.de/branchenbuch/staedte/schleswig-holstein/{county}/{city}/unternehmen\"\n",
    "    url = urllib.parse.quote(url, safe=':/')\n",
    "    print(url)\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    #get all the links to the buisnesses\n",
    "    links = soup.find_all('a',class_='link')\n",
    "    all_links = []\n",
    "    for elm in links:\n",
    "        link_yp = elm.get(\"href\") #get the yellowpage link for 1 buisness\n",
    "        #only keep relevant links\n",
    "        if link_yp.startswith('https://www.gelbeseiten.de/gsbiz'):\n",
    "            all_links.append(link_yp)\n",
    "\n",
    "    return all_links\n",
    "\n",
    "# ----- COLLECT ACTUAL YP BUSINESS INFO -----\n",
    "def collect_buisness_info_nittyGritty(my_buisnesses,my_list):\n",
    "    for elm in my_buisnesses:\n",
    "        time.sleep(1)\n",
    "        #for each element in list of all buisness per branch (in one city)\n",
    "        try:\n",
    "            html = requests.get(elm).text #go there\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            json_soup = soup.find_all(type=\"application/ld+json\") #same as above\n",
    "            if len(json_soup) > 0:\n",
    "                e = json_soup[len(json_soup)-1] #seems to be in last element always (i hope)\n",
    "                e = e.text\n",
    "                data = json.loads(e)\n",
    "            else:\n",
    "                data = {\"business\":elm, \"message\":\"failure - this buisness seems to not be available in json format\"}\n",
    "        except Exception as e:\n",
    "                error_type = type(e).__name__  # Get the name of the exception\n",
    "                error_message = str(e)         # Get the error message\n",
    "                data = {\"website\":elm, \"error_type\":error_type, \"error_message\":error_message}        \n",
    "        \n",
    "        my_list.append(data)\n",
    "\n",
    "\n",
    "# ----- EXPAND RESULT COUNT TO 50+ -----\n",
    "def expand_resultcount(url):\n",
    "    print(url)\n",
    "\n",
    "    #url = 'https://www.gelbeseiten.de/branchen/friseur/lübeck'\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(url)\n",
    "\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    #manage cookie button\n",
    "    try:\n",
    "        accept_button = driver.find_element(By.XPATH, \"/html/body/div[1]/div[1]/div[2]/span[1]/a\")\n",
    "        accept_button.click()\n",
    "    except:\n",
    "        accept_button = None\n",
    "\n",
    "    #find out how many results in total there are\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    result_totalCount = soup.find_all(id=\"loadMoreGesamtzahl\")\n",
    "    if len(result_totalCount) > 0:\n",
    "        result_totalCount = int(result_totalCount[0].text) #get str w/ number, make into int\n",
    "        result_totalCount = np.round((result_totalCount-50)/10) #because first 50 results are shown\n",
    "    else:\n",
    "        result_totalCount = 0 #f not more than 50 results available just skip while loop\n",
    "\n",
    "    #click load more button as often as needed (each time 10 more results)\n",
    "    load_more_clicked = 1\n",
    "    while load_more_clicked <= result_totalCount:\n",
    "        #driver.execute_script(\"arguments[0].scrollIntoView(true);\", loadMore_button) #thanks chat gtp\n",
    "        #loadMore_button.click()\n",
    "        loadMore_button = driver.find_element(By.CSS_SELECTOR, \"#mod-LoadMore--button\")\n",
    "        time.sleep(2)\n",
    "        driver.execute_script(\"arguments[0].click();\", loadMore_button) #use java script to execute button, since sometimes image ads or headers obscure view (see demo)\n",
    "        load_more_clicked += 1\n",
    "\n",
    "    #now use bs4 again since i already wrote this, just extract all hrefs\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    all_links = soup.find_all(id=re.compile(r'^treffer_\\d+')) #each buisness is a 'treffer' (=hit) followed by _ and some digits\n",
    "    my_buisnesses = [s.find(\"a\")[\"href\"] for s in all_links] #now get all hrefs for the buisnesses and return that list\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return my_buisnesses\n",
    "\n",
    "\n",
    "# ----- COMBINE ALL RESULTS PER BRANCHE + COLLECT EACHs INFOS -----\n",
    "def collect_buisness_infos_2(branchen):\n",
    "    buisnesslist = []\n",
    "    for b in branchen:\n",
    "        #something like this url = \"https://www.gelbeseiten.de/branchen/apotheke/kiel\" is the url then\n",
    "        \n",
    "        #use function with selenium to expand search results from 50 to x, then use bs4 to collect all the hrefs for the buisnesses and return them as a list\n",
    "        my_buisnesses = expand_resultcount(b)\n",
    "        #now go through that list and get info on each buisness\n",
    "        #use this broad try thing, because e.g. timeout errors occured at 126 elements and who knows which other issues might appear (also eduroam hates me)\n",
    "        collect_buisness_info_nittyGritty(my_buisnesses, buisnesslist)\n",
    "    return buisnesslist\n",
    "\n",
    "\n",
    "# ----- SET UP MONGO -----\n",
    "def set_up_mongo(client_str,database_str,collection_str):\n",
    "    client = MongoClient(client_str) #connect to mongodb client\n",
    "    db = client[database_str] #connect to database\n",
    "\n",
    "    existing_collections = db.list_collection_names() #check that dbs collections\n",
    "    if collection_str not in existing_collections:\n",
    "        db.create_collection(collection_str) #create collection if needed\n",
    "    \n",
    "    my_collection = db[collection_str] #connect to collection\n",
    "\n",
    "    return my_collection\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"client_str = 'mongodb://localhost:27017/'\n",
    "database_str = 'd2v2' #'webscraping_dataLabKiel'\n",
    "collection_str = 'yp_additionalID' #'yellow_pages'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collection = set_up_mongo('mongodb://localhost:27017/','webscraping_dataLabKiel','yellow_pages')\n",
    "collection = set_up_mongo('mongodb://localhost:27017','sh_data_collection','yp_kiel') #mongodb...\n",
    "\n",
    "\n",
    "for kfs in [\"kiel\"]: #\"lübeck\",\"flensburg\",\"neumünster\"\n",
    "    my_branch_list = total_branch_collector(\"kreisfrei\",kfs)\n",
    "    sh_unternehmen = collect_buisness_infos_2(my_branch_list) \n",
    "    collection.insert_many(sh_unternehmen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collection = set_up_mongo('mongodb://localhost:27017/','webscraping_dataLabKiel','yellow_pages')\n",
    "collection = set_up_mongo('mongodb://mongodb:27017','webscraping_dataLabKiel','yellow_pages')\n",
    "\n",
    "#take the time to check collection and insertion status\n",
    "start_time = time.time()\n",
    "\n",
    "#get all counties\n",
    "lk_stack = county_search()\n",
    "\n",
    "#for each county, get its cities\n",
    "for lk in lk_stack:\n",
    "    city = city_search(lk)\n",
    "    pattern = r\"[)(]\" #take care of unclean stuff, look like only brakcets are an issue\n",
    "    city_chic = [re.sub(pattern, '', e) for e in city]\n",
    "    for c in city_chic:\n",
    "        sh_unternehmen = [] #set up collector\n",
    "        my_branch_list = total_branch_collector(lk,c) #get list of all links of all the branchen which are available in 1 city alphabetically\n",
    "        if my_branch_list == []:\n",
    "            smolTownBuisness = tooSmolNoBranch(lk,c) #get list of all href of buisness for that place (if so small theres no branchen split up)\n",
    "            collect_buisness_info_nittyGritty(smolTownBuisness,sh_unternehmen) #get info on each buisness\n",
    "        else:\n",
    "            sh_unternehmen = collect_buisness_infos_2(my_branch_list) #just connect normally\n",
    "        \n",
    "        if sh_unternehmen != []:\n",
    "            collection.insert_many(sh_unternehmen) #insert data collected so far: for 1 city in 1 landkreis!\n",
    "            intm_time = time.time()\n",
    "            print(f\"Update at {np.round((intm_time-start_time)/60,3)}: for {c} in {lk}, data was inserted!\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = set_up_mongo('mongodb://localhost:27017/','webscraping_dataLabKiel','yellow_pages')\n",
    "#collection = set_up_mongo('mongodb://localhost:27017','sh_data_collection','yp_kiel') #mongodb...\n",
    "\n",
    "#collection = set_up_mongo('mongodb://localhost:27017/','webscraping_dataLabKiel','yellow_pages_clean')\n",
    "\n",
    "for kfs in [\"kiel\",\"lübeck\",\"flensburg\",\"neumünster\"]:\n",
    "    my_branch_list = total_branch_collector(\"kreisfrei\",kfs)\n",
    "    sh_unternehmen = collect_buisness_infos_2(my_branch_list) \n",
    "    collection.insert_many(sh_unternehmen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some issues:\n",
    "\n",
    "\n",
    "\n",
    "- **some business adress not actualy in locality where i found it** <br>\n",
    "but whats really weird is that when you go here (one of the cities in dithmarschen) https://www.gelbeseiten.de/branchenbuch/staedte/schleswig-holstein/dithmarschen/bargenstedt/unternehmen what you see is for example the Fritz Käppner GmbH. and its adresses is in surprise: Nuermberg. wtf\n",
    "- **number of businesses** <br> apparently theres 123000+ businesses in SH, so maybe use some hash function so i dont have to query them all https://www.schleswig-holstein.de/DE/landesregierung/themen/wirtschaft/mittelstand-handwerk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Store the JSON data in a file\n",
    "with open(\"data_YP.txt\", \"w\") as file:\n",
    "    json.dump(sh_unternehmen, file, indent=4)\n",
    "\n",
    "print(\"Data stored successfully!\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Store the JSON data in a file\n",
    "with open(\"data_YP.json\", \"w\") as file:\n",
    "    json.dump(sh_unternehmen, file,indent=4)\n",
    "\n",
    "print(\"Data stored successfully!\")\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datalab_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
